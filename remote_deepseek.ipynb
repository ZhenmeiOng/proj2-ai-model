{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.5 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%pip install -q airavata-python-sdk[notebook]\n",
    "import airavata_jupyter_magic\n",
    "\n",
    "%authenticate\n",
    "%request_runtime hpc_cpu --file=cybershuttle.yml --walltime=60 --use=NeuroData25VC2:cloud,expanse:shared,anvil:shared\n",
    "%switch_runtime hpc_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "def main():\n",
    "    # Step 1: Define model path\n",
    "    MODEL_NAME = \"/cybershuttle_data/airavata-courses-deepseek-chat/deepseek-ai/deepseek-math-7b-instruct\"\n",
    "\n",
    "    # Step 2: Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        local_files_only=True\n",
    "    )\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        local_files_only=True\n",
    "    )\n",
    "\n",
    "    generation_config = GenerationConfig.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        local_files_only=True\n",
    "    )\n",
    "    generation_config.pad_token_id = generation_config.eos_token_id\n",
    "\n",
    "    # Step 3: Interactive prompt loop\n",
    "    while True:\n",
    "        user_input = input(\"\\nEnter your prompt (or type 'exit' to quit): \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        # Step 4: Prepare chat messages\n",
    "        messages = [{\"role\": \"user\", \"content\": user_input}]\n",
    "\n",
    "        # Step 5: Tokenize using chat template\n",
    "        input_tensor = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "        # Create attention mask\n",
    "        attention_mask = input_tensor.ne(tokenizer.pad_token_id).long()\n",
    "\n",
    "        # Step 6: Generate model output\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_tensor,\n",
    "            attention_mask=attention_mask.to(model.device),\n",
    "            max_new_tokens=512,\n",
    "            pad_token_id=generation_config.pad_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        # Step 7: Decode and print model output\n",
    "        result = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)\n",
    "        print(\"\\nModel Output:\\n\", result.strip())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
