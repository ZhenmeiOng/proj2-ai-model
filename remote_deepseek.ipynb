{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.5 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%pip install -q airavata-python-sdk[notebook]\n",
    "import airavata_jupyter_magic\n",
    "\n",
    "%authenticate\n",
    "%request_runtime hpc_cpu --file=cybershuttle.yml --walltime=60 --use=NeuroData25VC2:cloud,expanse:shared,anvil:shared\n",
    "%switch_runtime hpc_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In [1]:\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "# === Configuration ===\n",
    "model_path = \"/cybershuttle_data/airavata-courses-deepseek-chat/deepseek-math-7b-instruct\"\n",
    "prompt = \"Compute the derivative of sin(x) * x^2.\"  #Change this prompt\n",
    "# === Load tokenizer & model (offline) ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,    # or torch.float16 if preferred\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True\n",
    ")\n",
    "model.config.use_cache = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = f\"User: {prompt}\\nAssistant:\"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "gen_config = GenerationConfig(\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "output_ids = model.generate(**inputs, generation_config=gen_config)\n",
    "# decode only the newly generated tokens\n",
    "generated = tokenizer.decode(\n",
    "    output_ids[0][ inputs[\"input_ids\"].shape[-1]: ],\n",
    "    skip_special_tokens=True\n",
    ").strip()\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"Response:\", generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
